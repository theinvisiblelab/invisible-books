{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-23T20:39:45.386832Z",
     "start_time": "2025-03-23T20:39:45.368904Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "input_file = \"processed_open_library_data_dump.csv\"\n",
    "output_file = \"cleaned_open_library_data.csv\"\n",
    "\n",
    "chunk_size = 10000\n",
    "column_names = [\"book_id\", \"revision\", \"timestamp\", \"json_data\"]\n",
    "\n",
    "first_chunk = not os.path.exists(output_file)\n",
    "chunk_count = 0\n",
    "\n",
    "logging.getLogger(\"distributed.utils_perf\").setLevel(logging.ERROR)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalize the data",
   "id": "fa818bc28a6bdc30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T01:01:20.862454Z",
     "start_time": "2025-03-21T23:06:07.557727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, names=column_names, header=None):\n",
    "    chunk_count += 1\n",
    "    print(f\"Processing chunk {chunk_count}...\", end=\"\\r\")\n",
    "\n",
    "    chunk[\"json_data\"] = chunk[\"json_data\"].apply(lambda x: json.loads(x) if isinstance(x, str) else {})\n",
    "    normalized_df = pd.json_normalize(chunk[\"json_data\"])\n",
    "    final_df = pd.concat([chunk.drop(columns=[\"json_data\"]), normalized_df], axis=1)\n",
    "\n",
    "    final_df.to_csv(output_file, mode=\"w\" if first_chunk else \"a\", index=False, header=first_chunk)\n",
    "    first_chunk = False\n",
    "\n",
    "    del chunk, final_df, normalized_df\n",
    "    gc.collect()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"\\nFinished processing! {chunk_count} chunks processed. Data saved to {output_file}.\")"
   ],
   "id": "63225baec5dc75bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5315...\r\n",
      "Finished processing! 5315 chunks processed. Data saved to cleaned_open_library_data.csv.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert list columns to strings",
   "id": "748cb82a090e71f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:39:55.528682Z",
     "start_time": "2025-03-23T20:39:55.521667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "input_file = \"cleaned_open_library_data.csv\"\n",
    "output_file = \"converted_open_library_data.csv\"\n",
    "\n",
    "chunk_size = 10000\n",
    "first_chunk = not os.path.exists(output_file)\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        return \", \".join(map(str, val)) if isinstance(val, list) else x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def safe_extract_keys(x):\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        return \", \".join(d[\"key\"] for d in val if isinstance(d, dict) and \"key\" in d) if isinstance(val, list) else x\n",
    "    except:\n",
    "        return x"
   ],
   "id": "f4e2ed8a1604ad85",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T23:53:15.344171Z",
     "start_time": "2025-03-23T20:39:56.697919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, on_bad_lines='skip'):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    print(f\"Post-processing chunk {chunk_count} (Total rows: {total_rows})...\", end=\"\\r\")\n",
    "\n",
    "    list_columns = [\"isbn_13\", \"isbn_10\", \"publishers\", \"oclc_numbers\"]\n",
    "    for col in list_columns:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].apply(safe_eval_list)\n",
    "\n",
    "    dict_list_columns = [\"authors\", \"works\"]\n",
    "    for col in dict_list_columns:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].apply(safe_extract_keys)\n",
    "\n",
    "    chunk.to_csv(output_file, mode=\"w\" if first_chunk else \"a\", index=False, header=first_chunk)\n",
    "    first_chunk = False\n",
    "\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"\\nPost-processing complete! {chunk_count} chunks and {total_rows} total rows saved to {output_file}.\")\n"
   ],
   "id": "ef9e85db56fe48f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing chunk 10623 (Total rows: 106226248)...\r\n",
      "Post-processing complete! 10623 chunks and 106226248 total rows saved to final_open_library_data.csv.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Drop unnecessary columns and convert isbn_10 to isbn_13",
   "id": "3aaee8c3523b3828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:55:13.037232Z",
     "start_time": "2025-03-24T00:24:42.233724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "\n",
    "input_file = 'final_open_library_data.csv'\n",
    "output_file = 'simplified_open_library_data.csv'\n",
    "\n",
    "chunk_size = 10000\n",
    "first_chunk = True\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "main_columns = [\n",
    "    'book_id', 'title', 'full_title', 'isbn_13', 'isbn_10', 'key', 'works',\n",
    "    'publishers', 'publish_date', 'publish_country', 'edition_name',\n",
    "    'authors', 'by_statement',\n",
    "    'subjects', 'genres', 'identifiers.goodreads',\n",
    "    'identifiers.google', 'identifiers.doi',\n",
    "    'identifiers.wikidata', 'identifiers.librarything', 'identifiers.better_world_books',\n",
    "    'url', 'first_sentence', 'description'\n",
    "]\n",
    "\n",
    "def isbn10_to_isbn13(isbn10):\n",
    "    isbn10 = str(isbn10)\n",
    "    if len(isbn10) != 10 or not isbn10[:-1].isdigit():\n",
    "        return ''\n",
    "    isbn13 = '978' + isbn10[:-1]\n",
    "    total = sum((3 if i % 2 else 1) * int(num) for i, num in enumerate(isbn13))\n",
    "    check_digit = (10 - (total % 10)) % 10\n",
    "    return isbn13 + str(check_digit)\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, on_bad_lines='skip'):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    print(f'Processing chunk {chunk_count} (Total rows: {total_rows})...', end='\\r')\n",
    "\n",
    "    chunk['isbn_13'] = chunk['isbn_10'].apply(isbn10_to_isbn13)\n",
    "    final_df = chunk[main_columns]\n",
    "    final_df.to_csv(output_file, mode='w' if first_chunk else 'a', index=False, header=first_chunk)\n",
    "    first_chunk = False\n",
    "\n",
    "    del chunk, final_df\n",
    "    gc.collect()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f'\\nFinished processing! {chunk_count} chunks processed. Data saved to {output_file}.')"
   ],
   "id": "b4ec4e158f9ed6dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 10623 (Total rows: 106226248)...\r\n",
      "Finished processing! 10623 chunks processed. Data saved to simplified_open_library_data.csv.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Keep one book for each work\n",
   "id": "188e3a742b801b70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:02:46.270199Z",
     "start_time": "2025-03-24T01:55:13.097973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "\n",
    "input_file = 'simplified_open_library_data.csv'\n",
    "output_file = 'unique_works_open_library_data.csv'\n",
    "\n",
    "chunk_size = 10000\n",
    "first_chunk = True\n",
    "chunk_count = 0\n",
    "\n",
    "def keep_one_book_per_work(chunk):\n",
    "    return chunk.drop_duplicates(subset=['works'], keep='first')\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, on_bad_lines='skip'):\n",
    "    chunk_count += 1\n",
    "    print(f'Processing chunk {chunk_count}...', end='\\r')\n",
    "\n",
    "    unique_chunk = keep_one_book_per_work(chunk)\n",
    "    unique_chunk.to_csv(output_file, mode='w' if first_chunk else 'a', index=False, header=first_chunk)\n",
    "    first_chunk = False\n",
    "\n",
    "    del chunk, unique_chunk\n",
    "    gc.collect()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f'\\nFinished processing! {chunk_count} chunks processed. Data saved to {output_file}.')"
   ],
   "id": "46545a21c0e34a0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 10623...\r\n",
      "Finished processing! 10623 chunks processed. Data saved to unique_works_open_library_data.csv.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "703aa4664370963"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
